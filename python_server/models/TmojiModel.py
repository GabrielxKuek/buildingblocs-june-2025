import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet
import re
import spacy


nlp = spacy.load("en_core_web_sm")

# Ensure that the necessary NLTK resources are downloaded
nltk.download('wordnet')
nltk.download('omw-1.4')

# Initialize lemmatizer
lemmatizer = WordNetLemmatizer()

# Emoji dictionary
comprehension_dict = {
  # Medical/Care instructions (expanded)
  "medicine": "ðŸ’Š", "pill": "ðŸ’Š", "medication": "ðŸ’Š", "take": "ðŸ‘‹",
  "doctor": "ðŸ‘¨â€âš•ï¸", "nurse": "ðŸ‘©â€âš•ï¸", "appointment": "ðŸ“…", "hospital": "ðŸ¥", "clinic": "ðŸ¥",
  "therapy": "ðŸ§ ", "exercise": "ðŸƒ", "physical": "ðŸƒ", "speech": "ðŸ—£ï¸",
  "blood": "ðŸ©¸", "pressure": "ðŸ©¸", "temperature": "ðŸŒ¡ï¸", "heart": "â¤ï¸", "pulse": "ðŸ’“",
  "xray": "ðŸ“·", "scan": "ðŸ“·", "test": "ðŸ”¬", "results": "ðŸ“„", "injection": "ðŸ’‰", "shot": "ðŸ’‰",
  "bandage": "ðŸ©¹", "cast": "ðŸ¦´", "crutches": "ðŸ©¼", "oxygen": "ðŸ’¨", "allergy": "ðŸ¤§",
  "dizzy": "ðŸ’«", "nausea": "ðŸ¤¢", "vomit": "ðŸ¤®", "itchy": "ðŸ¦Ÿ", "rash": "ðŸ”´", "cough": "ðŸ¤§",
  
  # Daily care instructions (expanded)
  "eat": "ðŸ½ï¸", "drink": "ðŸ¥¤", "water": "ðŸ’§", "food": "ðŸ½ï¸", "meal": "ðŸ½ï¸", "snack": "ðŸŽ",
  "breakfast": "ðŸŒ…ðŸ½ï¸", "lunch": "â˜€ï¸ðŸ½ï¸", "dinner": "ðŸŒ†ðŸ½ï¸", "dessert": "ðŸ°",
  "shower": "ðŸš¿", "bath": "ðŸ›", "wash": "ðŸ§¼", "brush": "ðŸ¦·", "teeth": "ðŸ¦·", "hair": "ðŸ’‡",
  "sleep": "ðŸ˜´", "nap": "ðŸ˜´", "rest": "ðŸ˜´", "bed": "ðŸ›ï¸", "awake": "ðŸ‘€", "wake": "â°",
  "bathroom": "ðŸš½", "toilet": "ðŸš½", "diaper": "ðŸ‘¶", "change": "ðŸ”„", "clean": "ðŸ§¹", "dirty": "ðŸ§¼",
  "clothes": "ðŸ‘•", "dress": "ðŸ‘—", "shirt": "ðŸ‘•", "pants": "ðŸ‘–", "shoes": "ðŸ‘Ÿ", "socks": "ðŸ§¦",
  "laundry": "ðŸ‘•ðŸŒ€", "dishes": "ðŸ½ï¸ðŸ§¼", "cook": "ðŸ‘©â€ðŸ³", "shop": "ðŸ›’", "drive": "ðŸš—", "work": "ðŸ’¼",
  
  # Basic Verbs (expanded)
  "go": "âž¡ï¸", "come": "â¬…ï¸", "stay": "â¹ï¸", "sit": "ðŸª‘", "stand": "ðŸ§", "walk": "ðŸš¶", "run": "ðŸƒ",
  "give": "ðŸ¤²", "take": "ðŸ‘‹", "get": "ðŸ«³", "put": "â¬‡ï¸", "bring": "âž¡ï¸", "carry": "ðŸ‹ï¸",
  "make": "ðŸ› ï¸", "do": "ðŸ”¨", "have": "ðŸ«´", "use": "ðŸ–ï¸", "need": "ðŸ™", "want": "ðŸ¤²",
  "try": "ðŸ”§", "find": "ðŸ”", "keep": "ðŸ“¥", "leave": "ðŸšª", "open": "ðŸ“‚", "close": "ðŸ“",
  "start": "â¯ï¸", "stop": "â¹ï¸", "wait": "â³", "help": "ðŸ†˜", "show": "ðŸ‘€", "look": "ðŸ‘€",
  "ask": "ðŸ—£ï¸", "tell": "ðŸ’¬", "talk": "ðŸ’¬", "speak": "ðŸ’¬", "listen": "ðŸ‘‚", "hear": "ðŸ‘‚",
  "read": "ðŸ“–", "write": "âœï¸", "draw": "ðŸŽ¨", "play": "ðŸŽ®", "watch": "ðŸ“º", "call": "ðŸ“ž",
  "feel": "ðŸ¤²", "hold": "ðŸ¤", "touch": "âœ‹", "hug": "ðŸ«‚", "love": "â¤ï¸", "like": "ðŸ‘",
  
  # Questions others ask
  "how": "â“", "what": "â“", "where": "â“", "when": "â“", "who": "â“", "why": "â“", "which": "â“",
  "feel": "ðŸ˜Š", "feeling": "ðŸ˜Š", "pain": "ðŸ˜£", "hurt": "ðŸ˜£", "okay": "ðŸ‘", "wrong": "âŒ",
  "need": "ðŸ¤²", "want": "ðŸ¤²", "like": "ðŸ‘", "comfortable": "ðŸ˜Œ", "ready": "âœ…",
  "hungry": "ðŸ½ï¸", "thirsty": "ðŸ˜°", "tired": "ðŸ˜´", "cold": "â„ï¸", "hot": "ðŸ”¥", "sick": "ðŸ¤’",
  
  # Time references
  "now": "â°", "today": "ðŸ“…", "tomorrow": "ðŸ“…", "yesterday": "ðŸ“…â¬…ï¸", 
  "morning": "ðŸŒ…", "afternoon": "â˜€ï¸", "evening": "ðŸŒ‡", "night": "ðŸŒ™", 
  "later": "â°", "soon": "â°", "early": "â°â¬†ï¸", "late": "â°â¬‡ï¸",
  "minute": "â°", "hour": "â°", "day": "ðŸ“…", "week": "ðŸ“…7ï¸âƒ£", "month": "ðŸ“…30ï¸âƒ£", "year": "ðŸ“…365ï¸âƒ£",
  
  # Family/visitor communication
  "family": "ðŸ‘ª", "visit": "ðŸ‘‹", "visitor": "ðŸ‘¥", "call": "ðŸ“ž", "phone": "ðŸ“±", "text": "ðŸ’¬",
  "mom": "ðŸ‘©", "dad": "ðŸ‘¨", "mother": "ðŸ‘©", "father": "ðŸ‘¨", "wife": "ðŸ‘©", "husband": "ðŸ‘¨",
  "son": "ðŸ‘¦", "daughter": "ðŸ‘§", "child": "ðŸ‘¶", "grandchild": "ðŸ‘¶", "baby": "ðŸ‘¶",
  "friend": "ðŸ‘«", "neighbor": "ðŸ ðŸ‘¥", "pet": "ðŸ•", "dog": "ðŸ•", "cat": "ðŸˆ",
  
  # Location/movement
  "go": "âž¡ï¸", "come": "â¬…ï¸", "stay": "â¹ï¸", "sit": "ðŸª‘", "stand": "ðŸ§", "walk": "ðŸš¶", "run": "ðŸƒ",
  "room": "ðŸ ", "kitchen": "ðŸ³", "bedroom": "ðŸ›ï¸", "living": "ðŸ›‹ï¸", "garden": "ðŸŒ³", "yard": "ðŸŒ±",
  "outside": "ðŸŒ³", "inside": "ðŸ ", "here": "ðŸ“", "there": "ðŸ“", "up": "â¬†ï¸", "down": "â¬‡ï¸",
  "home": "ðŸ ", "car": "ðŸš—", "wheelchair": "â™¿", "stairs": "ðŸªœ", "elevator": "ðŸ›—",
  "help": "ðŸ†˜", "emergency": "ðŸš¨", "911": "ðŸš¨", "fire": "ðŸ”¥", "police": "ðŸ‘®", "danger": "âš ï¸",
  "safe": "âœ…", "careful": "âš ï¸", "stop": "âœ‹", "fall": "âš ï¸", "hurt": "ðŸ¤•", "accident": "ðŸš‘",
  "love": "â¤ï¸", "care": "â¤ï¸", "worry": "ðŸ˜”", "worry": "ðŸ˜Ÿ", "scared": "ðŸ˜¨", "afraid": "ðŸ˜¨",
  "happy": "ðŸ˜„", "sad": "ðŸ˜¢", "angry": "ðŸ˜ ", "mad": "ðŸ˜ ", "calm": "ðŸ§˜", "relax": "ðŸ˜Œ",
  "better": "ðŸ“ˆ", "worse": "ðŸ“‰", "same": "âž¡ï¸", "improve": "ðŸ“ˆ", "heal": "â¤ï¸ðŸ©¹", "well": "ðŸ‘",
  "please": "ðŸ™", "can": "â“", "could": "â“", "would": "â“", "will": "â©", "shall": "â“",
  "open": "ðŸ“‚", "close": "ðŸ“", "turn": "ðŸ”„", "press": "ðŸ‘†", "push": "ðŸ‘†", "pull": "ðŸ‘‡",
  "give": "ðŸ¤²", "bring": "âž¡ï¸", "show": "ðŸ‘€", "look": "ðŸ‘€", "listen": "ðŸ‘‚", "wait": "â³",
  "yes": "âœ…", "no": "âŒ", "maybe": "ðŸ¤·", "ok": "ðŸ‘", "okay": "ðŸ‘", "alright": "ðŸ‘",
  "sure": "âœ…", "fine": "ðŸ‘", "good": "ðŸ‘", "bad": "ðŸ‘Ž", "cannot": "âŒ", "not": "âŒ",
  "coffee": "â˜•", "tea": "ðŸµ", "juice": "ðŸ§ƒ", "milk": "ðŸ¥›", "beer": "ðŸº", "wine": "ðŸ·",
  "soda": "ðŸ¥¤", "cup": "â˜•", "glass": "ðŸ¥›", "bottle": "ðŸ¾", "refresh": "ðŸ’¦", "ice": "ðŸ§Š",
  "watermelon": "ðŸ‰", "lemon": "ðŸ‹", "apple": "ðŸŽ", "banana": "ðŸŒ", "bread": "ðŸž", "rice": "ðŸš",
  "?": "â“", "!": "â—", "let's": "ðŸ¤", "you": "ðŸ«µ", "your": "ðŸ«µ", "me": "ðŸ‘ˆ", "my": "ðŸ‘ˆ",
  "money": "ðŸ’°", "keys": "ðŸ”‘", "wallet": "ðŸ‘›", "remote": "ðŸ“±", "light": "ðŸ’¡", "tv": "ðŸ“º",
  "book": "ðŸ“–", "game": "ðŸŽ®", "music": "ðŸŽµ", "party": "ðŸŽ‰", "gift": "ðŸŽ", "pray": "ðŸ™"
}

phrase_dict = {
  # Question patterns (lemmatized forms)
  "how be you": "â“ðŸ«µ",
  "what be": "â“",
  "where be": "â“ðŸ“",
  "when be": "â“â°",
  "who be": "â“ðŸ‘¤",
  "why be": "â“",
  
  # Negation patterns (lemmatized)
  "do not": "âŒ",
  "be not": "âŒ",
  "will not": "âŒâ©",
  "can not": "âŒ",
  
  # Modal patterns (lemmatized)
  "need to": "ðŸ™",
  "want to": "ðŸ™",
  "have to": "ðŸ“‹", 
  "go to": "âž¡ï¸",
  
  "take medicine": "ðŸ—£ï¸ðŸ’Š",
  "feel pain": "ðŸ˜£ðŸ’¢", 
  "call doctor": "ðŸ“žðŸ‘¨â€âš•ï¸",
  "go hospital": "âž¡ï¸ðŸ¥",
  
  "use bathroom": "ðŸš½ðŸ§»",
  "drink water": "ðŸ’§ðŸ¥¤",
  "eat food": "ðŸ½ï¸", 
  "get dress": "ðŸ‘•ðŸ‘–",
  "take shower": "ðŸš¿ðŸ§¼",
  "brush tooth": "ðŸª¥ðŸ¦·",
  
  "love you": "â¤ï¸ðŸ«µ",
  "miss you": "ðŸ˜¢ðŸ’­ðŸ«µ",
  "see you": "ðŸ‘€ðŸ‘‹", 
  "call family": "ðŸ“žðŸ‘ª",
  "visit today": "ðŸ‘‹ðŸ“…ðŸ ",

  "right now": "â°â—",
  "later today": "â°âž¡ï¸ðŸ“…",
  "every day": "ðŸ“…ðŸ”„",
  "once a": "1ï¸âƒ£ðŸ“…",
  "twice a": "2ï¸âƒ£ðŸ“…",

  "call 911": "ðŸ“žðŸš¨ðŸ‘®",
  "need help": "ðŸ™ðŸ†˜", 
  "feel sick": "ðŸ¤’ðŸ¤¢",
  "cannot breathe": "âŒðŸ«ðŸ˜«",
  "hurt bad": "ðŸ˜£ðŸ’¢â—",

  "feel good": "ðŸ˜ŠðŸ‘",
  "feel bad": "ðŸ˜¢ðŸ‘Ž",
  "be okay": "ðŸ‘âœ…",
  "be safe": "âœ…ðŸ›¡ï¸",
  "feel better": "ðŸ˜ŠðŸ“ˆâ¤ï¸",
  "do not": "âŒ",
  "can not": "âŒ",
  "do you": "â“ðŸ«µ",
  "are you": "â“ðŸ«µ",
  "can you": "â“ðŸ«µ",
  "will you": "â“ðŸ«µ",
}

contractions_dict = {
  # Specific irregular cases FIRST
  "won't": "will not",
  "can't": "cannot",
  "shan't": "shall not",
  "ain't": "am not",
  "ma'am": "madam",
  "o'clock": "of the clock",
  "y'all": "you all",
  
  # Informal contractions
  "gonna": "going to",
  "wanna": "want to", 
  "gotta": "got to",
  "hafta": "have to",
  "kinda": "kind of",
  "sorta": "sort of",
  "lotta": "lot of",
  "outta": "out of",
  "coulda": "could have",
  "shoulda": "should have",
  "woulda": "would have",
  "mighta": "might have",
  "musta": "must have",
  
  # Negative contractions (specific verbs first)
  "haven't": "have not",
  "hasn't": "has not",
  "hadn't": "had not",
  "wasn't": "was not",
  "weren't": "were not",
  "isn't": "is not",
  "aren't": "are not",
  "doesn't": "does not",
  "didn't": "did not",
  "don't": "do not",
  "wouldn't": "would not",
  "shouldn't": "should not",
  "couldn't": "could not",
  "mightn't": "might not",
  "mustn't": "must not",
  "needn't": "need not",
  "daren't": "dare not",
  "usedn't": "used not",
  
  # Perfect tense contractions
  "should've": "should have",
  "could've": "could have", 
  "would've": "would have",
  "might've": "might have",
  "must've": "must have",
  "ought've": "ought have",
  
  # Specific pronoun + verb contractions
  "that's": "that is",
  "there's": "there is",
  "here's": "here is",
  "what's": "what is",
  "where's": "where is",
  "when's": "when is",
  "how's": "how is",
  "who's": "who is",
  "why's": "why is",
  "it's": "it is",
  "he's": "he is",
  "she's": "she is",
  "we're": "we are",
  "they're": "they are",
  "you're": "you are",
  "let's": "let us",
  
  # Perfect tense (have)
  "you've": "you have",
  "we've": "we have", 
  "they've": "they have",
  "i've": "i have",
  
  # Future tense (will)
  "you'll": "you will",
  "we'll": "we will",
  "they'll": "they will", 
  "i'll": "i will",
  "he'll": "he will",
  "she'll": "she will",
  "it'll": "it will",
  "that'll": "that will",
  "there'll": "there will",
  
  # Conditional (would)
  "you'd": "you would",
  "we'd": "we would",
  "they'd": "they would",
  "i'd": "i would", 
  "he'd": "he would",
  "she'd": "she would",
  "it'd": "it would",
  "that'd": "that would",
  "there'd": "there would",
  
  # First person singular
  "i'm": "i am",
  
  # General patterns LAST (to avoid conflicts)
  "n't": " not",
  "'re": " are", 
  "'ve": " have",
  "'ll": " will",
  "'d": " would",
  "'m": " am",
  "'s": " is",
}

# --- Helper Functions ---
# Function to convert contractions into non contraction text
def fix_contractions(text):  
  for contraction, expansion in contractions_dict.items():
    text = text.replace(contraction, expansion)
    
  return text


# Function to get the best synonym for a word based on the comprehension dictionary
def get_best_synonym(word, pos_tag=None):
  if word in comprehension_dict:
    return word
  
  important_words = ['need', 'do', 'be', 'have', 'go', 'get', 'want']
  if word in important_words:
    return word
  
  synonyms = wordnet.synsets(word, pos=pos_tag)
  for syn in synonyms:
    for lemma in syn.lemma_names():
      lemma_clean = lemma.replace('_', ' ').lower()
      if lemma_clean in comprehension_dict:
        return lemma_clean
  
  return word # fallback to original if no match

# Function to preprocess text
def preprocess_text(argText):
  # Lower and fix contractions
  text = argText.lower() 
  text = fix_contractions(text)
  # print(f"After contractions: {text}")        # TO REMOVE
  
  # Clean and Lemmatize
  text = re.sub(r'[^\w\s\?\!]', '', text)  # remove punctuation
  doc = nlp(text)
  
  lemmatized_words = []
  pos_tags = []
  
  for token in doc:
    if not token.is_punct and token.is_alpha:
      lemmatized_words.append(token.lemma_.lower())
      pos_tags.append(token.pos_)
    elif token.text in ['?', '!']:
      lemmatized_words.append(token.text)
      pos_tags.append('PUNCT')

  # Synonym replacement
  replaced_words = []
  for i, word in enumerate(lemmatized_words):
    if word in ['?', '!']:
      replaced_words.append(word)
    else:
      pos_map = {'NOUN': wordnet.NOUN, 'VERB': wordnet.VERB, 
                'ADJ': wordnet.ADJ, 'ADV': wordnet.ADV}
      wordnet_pos = pos_map.get(pos_tags[i], None)
      synonym = get_best_synonym(word, wordnet_pos)
      replaced_words.append(synonym)
    
  processed_text = " ".join(replaced_words)
  # print(f"After processing: {processed_text}")
  
  return {
    "processed_text": processed_text,
    "replaced_words": replaced_words
  }
  

# --- Main Function ---
def convert_to_emojis(text):
  #  Preprocess the text
  processed_data = preprocess_text(text)
  processed_text = processed_data["processed_text"]
  words_list = processed_text.split()    
  
  result_array = words_list.copy()  # Same length as original
  used_positions = set()
 
  # phrase first (need find their position and mark them so, it wont get lost)
  for phrase in sorted(phrase_dict.keys(), key=len, reverse=True):
    phrase_words = phrase.split()
    
    # Find where this phrase starts in the words_list
    for i in range(len(words_list) - len(phrase_words) + 1):
      if words_list[i:i+len(phrase_words)] == phrase_words:
        print(f"Found phrase: {phrase} -> {phrase_dict[phrase]} at position {i}")
        # Replace the first word with the phrase emoji
        result_array[i] = phrase_dict[phrase]
        
        # Mark all positions in this phrase as used
        for j in range(i, i + len(phrase_words)):
          used_positions.add(j)
          
        # Remove the other words in the phrase (set to None)
        for j in range(i + 1, i + len(phrase_words)):
          result_array[j] = None
        break
            
  
  # Individual words (fill remaining positions with individual word emojis)
  for i, word in enumerate(words_list):
    if i not in used_positions:  # Position not used by a phrase
      if word in comprehension_dict:
        result_array[i] = comprehension_dict[word]
        print(f"Found word: {word} -> {comprehension_dict[word]} at position {i}")
      else:
        result_array[i] = word
        # print(f"No emoji for: {word} -> keeping as '{word}' at position {i}")
  
  final_result = [item for item in result_array if item is not None]
  # result = " ".join(emoji_sequence)
  # print(f'Final result: {result}')
  return " ".join(final_result)


# test_sentences = [
#   "It's time to take your medicine",
#   "Do you need to use the bathroom?", 
#   "How are you feeling today?",
#   "Your daughter is coming to visit",
#   "Can you drink some water please?",
#   "The doctor wants to see you",
#   "Are you having any pain?",
#   "Let's get you dressed now",
#   "Help is coming, don't worry!",
#   "What would you like to eat?"
# ]

# for sentence in test_sentences:
#   print(sentence)
#   result = convert_to_emojis(sentence)
#   print()
#   print(result)
  


































# ===== 5th Attempt (synonymous word tracking) =====
# --- Preprocessing ---
# Change the synonymus words to the words in the emoji_words dictionary
# def get_best_synonym(word, emoji_dict):
#   synonyms = wordnet.synsets(word)
#   for syn in synonyms:
#     for lemma in syn.lemma_names():
#       if lemma in emoji_dict:
#         return lemma
#   return word  # fallback to original if no match

# def remove_contractions(sentence):
#     return contractions.fix(sentence.lower())

# # --- Main Function ---
# def sentence_to_emoji(sentence):
#   doc = nlp(sentence.lower())
#   lemmatized_words = [token.lemma_ for token in doc]

#   replaced_words = [
#     get_best_synonym(word, emoji_phrases) for word in lemmatized_words
#   ]

#   # Reconstruct lemmatized sentence
#   lemmatized_sentence = " ".join(replaced_words)
#   output = []

#   for phrase in sorted(emoji_phrases, key=len, reverse=True):
#     if phrase in lemmatized_sentence:
#       output.append(emoji_phrases[phrase])
#       lemmatized_sentence = lemmatized_sentence.replace(phrase, "")  # avoid duplicate
      
#   return " ".join(output)



# print(sentence_to_emoji("I want a cold drink and a glass of milk"))
# print(sentence_to_emoji("Shall we drink water or have some hot coffee?"))

# test_sentences = [
#   "I am thirsty and want cold drinks",
#   "Let's have some coffee and tea",
#   "Party with beer and wine tonight",
#   "Morning juice and a glass of milk",
# ]

# for sent in test_sentences:
#   emojis = sentence_to_emoji(sent)
#   print(f"Sentence: {sent}\nEmojis: {emojis}\n")


# ===== 4th Attempt (Lemmatization included) =====
# # Function to lemmatize a phrase
# def lemmatize_phrase(phrase):
#   return " ".join([lemmatizer.lemmatize(w) for w in phrase.split()])

# def sentence_to_emoji(sentence):
#   sentence = sentence.lower()
#   sentence = re.sub(r'[^\w\s]', '', sentence)  # remove punctuation

#   words = sentence.split()
#   output = sentence

#   # Create all possible 2â€“4 word phrases
#   matched_phrases = {}
#   for n in range(4, 1, -1):  # 4, 3, 2
#     for i in range(len(words) - n + 1):
#       phrase = " ".join(words[i:i+n])
#       lemmatized = lemmatize_phrase(phrase)
#       if lemmatized in emoji_phrases:
#         matched_phrases[phrase] = emoji_phrases[lemmatized]

#   # Replace phrases first (longest ones first)
#   for phrase, emoji in matched_phrases.items():
#     output = output.replace(phrase, emoji)

#   # Lemmatize and replace single words
#   final_words = []
#   for word in output.split():
#     lemma = lemmatizer.lemmatize(word)
#     final_words.append(emoji_words.get(lemma, word))

#   return " ".join(final_words)



# ===== 5th Attempt (Phrase first approach) =====
# def sentence_to_emoji(sentence):
#   # Normalize the sentence
#   sentence = sentence.lower()
  
#   # Handle phrases first
#   for phrase, emoji in emoji_phrases.items():
#     if phrase in sentence:
#       sentence = sentence.replace(phrase, emoji)
      
#   # Lemmatize each word (handles Plurals like 'drinks' -> 'drink')
#   words = sentence.split()
#   translated = []
#   for word in words: 
#     lemma = lemmatizer.lemmatize(word)
#     translated.append(emoji_words.get(lemma, word))
    
#   return " ".join(translated)


# ===== 2nd attempt (words & phrases) ======
# def sentence_to_emoji(sentence):
#   sentence = sentence.lower()
#   output = sentence

#   # Handle phrases first
#   for phrase, emoji in emoji_phrases.items():
#     if phrase in output:
#       output = output.replace(phrase, emoji)

#   # Then replace single words
#   words = output.split()
#   translated = [emoji_words.get(word, word) for word in words]
#   return " ".join(translated)


# ===== 1st attempt (words) =====
# # --- Helper functions ---
# def normalize_word(word):
#   return word.lower()


# def sentence_to_emoji(sentence):
#   words = sentence.split()
#   emojis = []
#   for word in words:
#     norm_word = normalize_word(word)
#     if norm_word in emoji_dict:
#       emojis.append(emoji_dict[norm_word])
#   return ' '.join(emojis)


# # Test sentecnes with beverages focus
# test_sentences = [
#   "I am thirsty and want a cold drink",
#   "Let's have some coffee and tea",
#   "Party with beer and wine tonight",
#   "Morning juice and a glass of milk",
# ]

# for sent in test_sentences:
#   emojis = sentence_to_emoji(sent)
#   print(f"Sentence: {sent}\nEmojis: {emojis}\n")
